# RE_improved_baseline

This is a modified version
of [Improved Baseline for Sentence-level Relation Extraction](https://arxiv.org/abs/2102.01373) for the AACL 2022 paper.
The original code can be found [here](https://github.com/wzhouad/RE_improved_baseline).

We mainly focus on using IBRE for relation extraction tasks. We implements some debiasing methods for relation
classification tasks including:

* default: the original model
* EntityMask: mask the entity mentions in the input
* DataAug: data augmentation by replacing entity mentions with other entities
* RDrop: regularize the model by feedforward twice
* Focal: focal loss
* DFocal: Debiased Focal Loss
* PoE: Product-of-Expert used to integrate the predictions of target model and bias model
* Debias: Casual Debias Approach proposed by us
* RDataAug: Regularized Debias Approach proposed by us
* MixDebias: debiasing method proposed by us

## Requirements

* torch >= 1.8.1
* transformers >= 3.4.0
* wandb
* ujson
* tqdm


The Pytorch version must be at least 1.8.1 as our code relies on the both the ``torch.cuda.amp`` and
the ``torch.utils.checkpoint``, which are introduced in the 1.8.1 release.

## Usage

### Setup

You should prepare TACRED series datasets before running the code. The TACRED and Re-TACRED datasets can be downloaded
from the LDC links. After downloading the datasets, you should create the softlink to the datasets by running the
following command:

```bash
bash scripts/setup.sh
```

### Training

```bash
bash scripts/train.sh 0 tacred default
```

In the above command, the first argument is the GPU index, the second argument is the dataset name, and the third
argument is the debiasing method introduced before (default, EntityMask, DataAug, RDrop, Focal, DFocal, PoE, Debias,
RDataAug, MixDebias). There are some super parameters can be adjusted, you should look up the scripts.

### Evaluating

```bash
bash scripts/eval.sh 0 tacred default test
```

The above command will evaluate the perfomance of relation classification model trained on tacred dataset under default
seting on test split. 

By default the probability generated by model during predict progress will be stored for CoRE and
challenge dataset generation. You can disable the ability by challenge some variables defined in the scripts/eval.sh.
